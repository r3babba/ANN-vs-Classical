{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment IV– Individual Take Home Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. ANN regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "#Import Variables & standardize\n",
    "dataset=np.loadtxt(\"data/school_grades.csv\", delimiter=\",\", skiprows=1)\n",
    "x=dataset[:,0:4]\n",
    "y=dataset[:,4]\n",
    "y=np.reshape(y, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining a 3 layer deep NN [12,8,1]\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean_squared_error (mse) and mean_absolute_error (mae) are our loss functions – i.e. an estimate of how accurate the neural network is in predicting the test data. \n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 236 samples, validate on 60 samples\n",
      "Epoch 1/150\n",
      "236/236 [==============================] - 0s 850us/sample - loss: 0.3083 - mse: 0.3083 - mae: 0.5139 - val_loss: 0.3111 - val_mse: 0.3111 - val_mae: 0.5267\n",
      "Epoch 2/150\n",
      "236/236 [==============================] - 0s 78us/sample - loss: 0.2760 - mse: 0.2760 - mae: 0.4868 - val_loss: 0.2763 - val_mse: 0.2763 - val_mae: 0.4952\n",
      "Epoch 3/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.2454 - mse: 0.2454 - mae: 0.4593 - val_loss: 0.2444 - val_mse: 0.2444 - val_mae: 0.4644\n",
      "Epoch 4/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.2170 - mse: 0.2170 - mae: 0.4317 - val_loss: 0.2151 - val_mse: 0.2151 - val_mae: 0.4341\n",
      "Epoch 5/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.1909 - mse: 0.1909 - mae: 0.4049 - val_loss: 0.1871 - val_mse: 0.1871 - val_mae: 0.4029\n",
      "Epoch 6/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.1655 - mse: 0.1655 - mae: 0.3764 - val_loss: 0.1602 - val_mse: 0.1602 - val_mae: 0.3703\n",
      "Epoch 7/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.1402 - mse: 0.1402 - mae: 0.3455 - val_loss: 0.1304 - val_mse: 0.1304 - val_mae: 0.3307\n",
      "Epoch 8/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.1142 - mse: 0.1142 - mae: 0.3088 - val_loss: 0.1018 - val_mse: 0.1018 - val_mae: 0.2871\n",
      "Epoch 9/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0893 - mse: 0.0893 - mae: 0.2697 - val_loss: 0.0773 - val_mse: 0.0773 - val_mae: 0.2440\n",
      "Epoch 10/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0683 - mse: 0.0683 - mae: 0.2318 - val_loss: 0.0576 - val_mse: 0.0576 - val_mae: 0.2048\n",
      "Epoch 11/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0525 - mse: 0.0525 - mae: 0.1985 - val_loss: 0.0425 - val_mse: 0.0425 - val_mae: 0.1715\n",
      "Epoch 12/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0418 - mse: 0.0418 - mae: 0.1702 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1448\n",
      "Epoch 13/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0340 - mse: 0.0340 - mae: 0.1471 - val_loss: 0.0251 - val_mse: 0.0251 - val_mae: 0.1255\n",
      "Epoch 14/150\n",
      "236/236 [==============================] - 0s 68us/sample - loss: 0.0294 - mse: 0.0294 - mae: 0.1319 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.1127\n",
      "Epoch 15/150\n",
      "236/236 [==============================] - 0s 89us/sample - loss: 0.0268 - mse: 0.0268 - mae: 0.1216 - val_loss: 0.0190 - val_mse: 0.0190 - val_mae: 0.1047\n",
      "Epoch 16/150\n",
      "236/236 [==============================] - 0s 76us/sample - loss: 0.0254 - mse: 0.0254 - mae: 0.1164 - val_loss: 0.0176 - val_mse: 0.0176 - val_mae: 0.0999\n",
      "Epoch 17/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0243 - mse: 0.0243 - mae: 0.1124 - val_loss: 0.0165 - val_mse: 0.0165 - val_mae: 0.0957\n",
      "Epoch 18/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.0233 - mse: 0.0233 - mae: 0.1094 - val_loss: 0.0155 - val_mse: 0.0155 - val_mae: 0.0927\n",
      "Epoch 19/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0224 - mse: 0.0224 - mae: 0.1068 - val_loss: 0.0147 - val_mse: 0.0147 - val_mae: 0.0897\n",
      "Epoch 20/150\n",
      "236/236 [==============================] - 0s 68us/sample - loss: 0.0214 - mse: 0.0214 - mae: 0.1039 - val_loss: 0.0138 - val_mse: 0.0138 - val_mae: 0.0864\n",
      "Epoch 21/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0205 - mse: 0.0205 - mae: 0.1012 - val_loss: 0.0129 - val_mse: 0.0129 - val_mae: 0.0836\n",
      "Epoch 22/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0195 - mse: 0.0195 - mae: 0.0982 - val_loss: 0.0120 - val_mse: 0.0120 - val_mae: 0.0804\n",
      "Epoch 23/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0187 - mse: 0.0187 - mae: 0.0951 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0774\n",
      "Epoch 24/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0179 - mse: 0.0179 - mae: 0.0925 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0751\n",
      "Epoch 25/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0170 - mse: 0.0170 - mae: 0.0899 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0733\n",
      "Epoch 26/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0163 - mse: 0.0163 - mae: 0.0875 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0705\n",
      "Epoch 27/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0156 - mse: 0.0156 - mae: 0.0851 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0683\n",
      "Epoch 28/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0149 - mse: 0.0149 - mae: 0.0827 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0659\n",
      "Epoch 29/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0142 - mse: 0.0142 - mae: 0.0803 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0636\n",
      "Epoch 30/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0137 - mse: 0.0137 - mae: 0.0779 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0612\n",
      "Epoch 31/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0131 - mse: 0.0131 - mae: 0.0755 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0590\n",
      "Epoch 32/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0126 - mse: 0.0126 - mae: 0.0731 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0568\n",
      "Epoch 33/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0121 - mse: 0.0121 - mae: 0.0710 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0557\n",
      "Epoch 34/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0116 - mse: 0.0116 - mae: 0.0699 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0556\n",
      "Epoch 35/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.0113 - mse: 0.0113 - mae: 0.0687 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0547\n",
      "Epoch 36/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0109 - mse: 0.0109 - mae: 0.0669 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0528\n",
      "Epoch 37/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0106 - mse: 0.0106 - mae: 0.0653 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0520\n",
      "Epoch 38/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0103 - mse: 0.0103 - mae: 0.0643 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0518\n",
      "Epoch 39/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0100 - mse: 0.0100 - mae: 0.0637 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0516\n",
      "Epoch 40/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0098 - mse: 0.0098 - mae: 0.0633 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0512\n",
      "Epoch 41/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0096 - mse: 0.0096 - mae: 0.0629 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0507\n",
      "Epoch 42/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0094 - mse: 0.0094 - mae: 0.0621 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0495\n",
      "Epoch 43/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0093 - mse: 0.0093 - mae: 0.0615 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0490\n",
      "Epoch 44/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0091 - mse: 0.0091 - mae: 0.0613 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0487\n",
      "Epoch 45/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0090 - mse: 0.0090 - mae: 0.0611 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0481\n",
      "Epoch 46/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0089 - mse: 0.0089 - mae: 0.0608 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0479\n",
      "Epoch 47/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0088 - mse: 0.0088 - mae: 0.0610 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0486\n",
      "Epoch 48/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0088 - mse: 0.0088 - mae: 0.0614 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0484\n",
      "Epoch 49/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0087 - mse: 0.0087 - mae: 0.0611 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0478\n",
      "Epoch 50/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0087 - mse: 0.0087 - mae: 0.0607 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0472\n",
      "Epoch 51/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0086 - mse: 0.0086 - mae: 0.0602 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0473\n",
      "Epoch 52/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0085 - mse: 0.0085 - mae: 0.0603 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0482\n",
      "Epoch 53/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0085 - mse: 0.0085 - mae: 0.0614 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0496\n",
      "Epoch 54/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0086 - mse: 0.0086 - mae: 0.0626 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0500\n",
      "Epoch 55/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0085 - mse: 0.0085 - mae: 0.0616 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0476\n",
      "Epoch 56/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0085 - mse: 0.0085 - mae: 0.0599 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0476\n",
      "Epoch 57/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0085 - mse: 0.0085 - mae: 0.0611 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0495\n",
      "Epoch 58/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0084 - mse: 0.0084 - mae: 0.0613 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0490\n",
      "Epoch 59/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0084 - mse: 0.0084 - mae: 0.0609 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0486\n",
      "Epoch 60/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0083 - mse: 0.0083 - mae: 0.0597 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0474\n",
      "Epoch 61/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0084 - mse: 0.0084 - mae: 0.0596 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0484\n",
      "Epoch 62/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0083 - mse: 0.0083 - mae: 0.0604 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0489\n",
      "Epoch 63/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0083 - mse: 0.0083 - mae: 0.0606 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0487\n",
      "Epoch 64/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0083 - mse: 0.0083 - mae: 0.0603 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0484\n",
      "Epoch 65/150\n",
      "236/236 [==============================] - 0s 34us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0598 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0482\n",
      "Epoch 66/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0599 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0483\n",
      "Epoch 67/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0594 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0477\n",
      "Epoch 68/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0590 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0473\n",
      "Epoch 69/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0597 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0488\n",
      "Epoch 70/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0607 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0491\n",
      "Epoch 71/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0601 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0479\n",
      "Epoch 72/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0591 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0474\n",
      "Epoch 73/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0587 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0473\n",
      "Epoch 74/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0595 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0487\n",
      "Epoch 75/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0082 - mse: 0.0082 - mae: 0.0602 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0484\n",
      "Epoch 76/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0599 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0481\n",
      "Epoch 77/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0586 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0468\n",
      "Epoch 78/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0581 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0469\n",
      "Epoch 79/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0589 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0486\n",
      "Epoch 80/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0602 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0485\n",
      "Epoch 81/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0590 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0471\n",
      "Epoch 82/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0589 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0477\n",
      "Epoch 83/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0588 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0471\n",
      "Epoch 84/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0081 - mse: 0.0081 - mae: 0.0580 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0461\n",
      "Epoch 85/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0579 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0475\n",
      "Epoch 86/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0593 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0485\n",
      "Epoch 87/150\n",
      "236/236 [==============================] - 0s 34us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0591 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0472\n",
      "Epoch 88/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0581 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0462\n",
      "Epoch 89/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0575 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0462\n",
      "Epoch 90/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0584 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0477\n",
      "Epoch 91/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0586 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0468\n",
      "Epoch 92/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0080 - mse: 0.0080 - mae: 0.0577 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0462\n",
      "Epoch 93/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0577 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0468\n",
      "Epoch 94/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0577 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0464\n",
      "Epoch 95/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0579 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0470\n",
      "Epoch 96/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0579 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0462\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0574 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0465\n",
      "Epoch 98/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0580 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0469\n",
      "Epoch 99/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0579 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0465\n",
      "Epoch 100/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0575 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0459\n",
      "Epoch 101/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0568 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0457\n",
      "Epoch 102/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0575 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0466\n",
      "Epoch 103/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0572 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0456\n",
      "Epoch 104/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0567 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0460\n",
      "Epoch 105/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0573 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0462\n",
      "Epoch 106/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0576 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0459\n",
      "Epoch 107/150\n",
      "236/236 [==============================] - 0s 34us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0569 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0451\n",
      "Epoch 108/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0562 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0450\n",
      "Epoch 109/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0079 - mse: 0.0079 - mae: 0.0568 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0452\n",
      "Epoch 110/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0560 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0440\n",
      "Epoch 111/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0553 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0441\n",
      "Epoch 112/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0560 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0451\n",
      "Epoch 113/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0569 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0446\n",
      "Epoch 114/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0556 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0439\n",
      "Epoch 115/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0552 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0442\n",
      "Epoch 116/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0558 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0450\n",
      "Epoch 117/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0569 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0451\n",
      "Epoch 118/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0559 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0439\n",
      "Epoch 119/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0548 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0432\n",
      "Epoch 120/150\n",
      "236/236 [==============================] - 0s 34us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0554 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0457\n",
      "Epoch 121/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0568 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0449\n",
      "Epoch 122/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0554 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0438\n",
      "Epoch 123/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0552 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0444\n",
      "Epoch 124/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0557 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0446\n",
      "Epoch 125/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0556 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0439\n",
      "Epoch 126/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0547 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0438\n",
      "Epoch 127/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0552 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0449\n",
      "Epoch 128/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0563 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0446\n",
      "Epoch 129/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0550 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0434\n",
      "Epoch 130/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0543 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0440\n",
      "Epoch 131/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0557 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0454\n",
      "Epoch 132/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0561 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0433\n",
      "Epoch 133/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0547 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0444\n",
      "Epoch 134/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0554 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0447\n",
      "Epoch 135/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0553 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0440\n",
      "Epoch 136/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0558 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0448\n",
      "Epoch 137/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0549 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0433\n",
      "Epoch 138/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0078 - mse: 0.0078 - mae: 0.0538 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0431\n",
      "Epoch 139/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0550 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0455\n",
      "Epoch 140/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0570 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0448\n",
      "Epoch 141/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0550 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0430\n",
      "Epoch 142/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0539 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0435\n",
      "Epoch 143/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0544 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0441\n",
      "Epoch 144/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0548 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0438\n",
      "Epoch 145/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0546 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0436\n",
      "Epoch 146/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0552 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0443\n",
      "Epoch 147/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0551 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0433\n",
      "Epoch 148/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0541 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0431\n",
      "Epoch 149/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0550 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0441\n",
      "Epoch 150/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0077 - mse: 0.0077 - mae: 0.0546 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0432\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# plotting loss function with epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled MSE: 0.015257056308782718\n",
      "MSE: 105.89927771966377\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "MSE_scaled = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "MSE = mean_squared_error(y_test, scaler_y.inverse_transform(y_pred))  \n",
    "\n",
    "print(\"Scaled MSE:\",MSE_scaled)\n",
    "print(\"MSE:\",MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Multiple Linear Regression Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "df = pd.read_csv('data/school_grades.csv')\n",
    "\n",
    "#making copy to work with\n",
    "df_copy = df.copy()\n",
    "\n",
    "X = df_copy.drop(['G3'], axis=1)\n",
    "y = df_copy[\"G3\"]\n",
    "\n",
    "#splitting training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   R-squared:                       0.825\n",
      "Model:                            OLS   Adj. R-squared:                  0.823\n",
      "Method:                 Least Squares   F-statistic:                     319.9\n",
      "Date:                Thu, 27 Feb 2020   Prob (F-statistic):          2.57e-101\n",
      "Time:                        00:00:55   Log-Likelihood:                -570.03\n",
      "No. Observations:                 276   AIC:                             1150.\n",
      "Df Residuals:                     271   BIC:                             1168.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.1265      1.667      0.676      0.500      -2.155       4.408\n",
      "age           -0.1917      0.097     -1.969      0.050      -0.383   -5.63e-05\n",
      "absences       0.0560      0.017      3.247      0.001       0.022       0.090\n",
      "G1             0.1375      0.067      2.066      0.040       0.006       0.268\n",
      "G2             0.9955      0.059     16.734      0.000       0.878       1.113\n",
      "==============================================================================\n",
      "Omnibus:                      151.110   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              788.130\n",
      "Skew:                          -2.289   Prob(JB):                    7.24e-172\n",
      "Kurtosis:                       9.897   Cond. No.                         340.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Fit regression model\n",
    "results = smf.ols('G3 ~ age + absences + G1 + G2', data=pd.concat([X_train,y_train], axis=1)).fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.5066709721080156\n"
     ]
    }
   ],
   "source": [
    "y_pred = results.predict(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred) \n",
    "\n",
    "print(\"MSE:\",MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
