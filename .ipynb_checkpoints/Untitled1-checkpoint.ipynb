{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment IV– Individual Take Home Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. ANN regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rahul Kalubowila\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "#Import Variables & standardize\n",
    "dataset=np.loadtxt(\"data/school_grades.csv\", delimiter=\",\", skiprows=1)\n",
    "x=dataset[:,0:4]\n",
    "y=dataset[:,4]\n",
    "y=np.reshape(y, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining a 3 layer deep NN [12,8,1]\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean_squared_error (mse) and mean_absolute_error (mae) are our loss functions – i.e. an estimate of how accurate the neural network is in predicting the test data. \n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 236 samples, validate on 60 samples\n",
      "Epoch 1/150\n",
      "236/236 [==============================] - 0s 893us/sample - loss: 0.4011 - mse: 0.4011 - mae: 0.5779 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.5990\n",
      "Epoch 2/150\n",
      "236/236 [==============================] - 0s 83us/sample - loss: 0.3332 - mse: 0.3332 - mae: 0.5217 - val_loss: 0.3550 - val_mse: 0.3550 - val_mae: 0.5522\n",
      "Epoch 3/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.2907 - mse: 0.2907 - mae: 0.4881 - val_loss: 0.3231 - val_mse: 0.3231 - val_mae: 0.5283\n",
      "Epoch 4/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.2715 - mse: 0.2715 - mae: 0.4736 - val_loss: 0.3114 - val_mse: 0.3114 - val_mae: 0.5190\n",
      "Epoch 5/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.2614 - mse: 0.2614 - mae: 0.4653 - val_loss: 0.2995 - val_mse: 0.2995 - val_mae: 0.5088\n",
      "Epoch 6/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.2498 - mse: 0.2498 - mae: 0.4552 - val_loss: 0.2852 - val_mse: 0.2852 - val_mae: 0.4964\n",
      "Epoch 7/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.2372 - mse: 0.2372 - mae: 0.4441 - val_loss: 0.2703 - val_mse: 0.2703 - val_mae: 0.4832\n",
      "Epoch 8/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.2244 - mse: 0.2244 - mae: 0.4322 - val_loss: 0.2563 - val_mse: 0.2563 - val_mae: 0.4702\n",
      "Epoch 9/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.2120 - mse: 0.2120 - mae: 0.4206 - val_loss: 0.2426 - val_mse: 0.2426 - val_mae: 0.4571\n",
      "Epoch 10/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.2003 - mse: 0.2003 - mae: 0.4092 - val_loss: 0.2289 - val_mse: 0.2289 - val_mae: 0.4436\n",
      "Epoch 11/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.1883 - mse: 0.1883 - mae: 0.3970 - val_loss: 0.2150 - val_mse: 0.2150 - val_mae: 0.4295\n",
      "Epoch 12/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.1763 - mse: 0.1763 - mae: 0.3842 - val_loss: 0.2011 - val_mse: 0.2011 - val_mae: 0.4148\n",
      "Epoch 13/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.1642 - mse: 0.1642 - mae: 0.3710 - val_loss: 0.1872 - val_mse: 0.1872 - val_mae: 0.3995\n",
      "Epoch 14/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.1521 - mse: 0.1521 - mae: 0.3569 - val_loss: 0.1734 - val_mse: 0.1734 - val_mae: 0.3836\n",
      "Epoch 15/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.1402 - mse: 0.1402 - mae: 0.3424 - val_loss: 0.1597 - val_mse: 0.1597 - val_mae: 0.3672\n",
      "Epoch 16/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.1293 - mse: 0.1293 - mae: 0.3282 - val_loss: 0.1461 - val_mse: 0.1461 - val_mae: 0.3504\n",
      "Epoch 17/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.1178 - mse: 0.1178 - mae: 0.3123 - val_loss: 0.1330 - val_mse: 0.1330 - val_mae: 0.3334\n",
      "Epoch 18/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.1071 - mse: 0.1071 - mae: 0.2964 - val_loss: 0.1204 - val_mse: 0.1204 - val_mae: 0.3158\n",
      "Epoch 19/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0969 - mse: 0.0969 - mae: 0.2802 - val_loss: 0.1083 - val_mse: 0.1083 - val_mae: 0.2979\n",
      "Epoch 20/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0873 - mse: 0.0873 - mae: 0.2645 - val_loss: 0.0969 - val_mse: 0.0969 - val_mae: 0.2796\n",
      "Epoch 21/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0786 - mse: 0.0786 - mae: 0.2481 - val_loss: 0.0861 - val_mse: 0.0861 - val_mae: 0.2612\n",
      "Epoch 22/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0705 - mse: 0.0705 - mae: 0.2328 - val_loss: 0.0763 - val_mse: 0.0763 - val_mae: 0.2436\n",
      "Epoch 23/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0636 - mse: 0.0636 - mae: 0.2184 - val_loss: 0.0674 - val_mse: 0.0674 - val_mae: 0.2261\n",
      "Epoch 24/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0571 - mse: 0.0571 - mae: 0.2046 - val_loss: 0.0595 - val_mse: 0.0595 - val_mae: 0.2101\n",
      "Epoch 25/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0518 - mse: 0.0518 - mae: 0.1913 - val_loss: 0.0528 - val_mse: 0.0528 - val_mae: 0.1949\n",
      "Epoch 26/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0475 - mse: 0.0475 - mae: 0.1794 - val_loss: 0.0471 - val_mse: 0.0471 - val_mae: 0.1825\n",
      "Epoch 27/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0442 - mse: 0.0442 - mae: 0.1689 - val_loss: 0.0424 - val_mse: 0.0424 - val_mae: 0.1716\n",
      "Epoch 28/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0417 - mse: 0.0417 - mae: 0.1609 - val_loss: 0.0388 - val_mse: 0.0388 - val_mae: 0.1617\n",
      "Epoch 29/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0400 - mse: 0.0400 - mae: 0.1546 - val_loss: 0.0359 - val_mse: 0.0359 - val_mae: 0.1529\n",
      "Epoch 30/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0388 - mse: 0.0388 - mae: 0.1498 - val_loss: 0.0338 - val_mse: 0.0338 - val_mae: 0.1464\n",
      "Epoch 31/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0379 - mse: 0.0379 - mae: 0.1462 - val_loss: 0.0323 - val_mse: 0.0323 - val_mae: 0.1415\n",
      "Epoch 32/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0373 - mse: 0.0373 - mae: 0.1437 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1378\n",
      "Epoch 33/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0369 - mse: 0.0369 - mae: 0.1418 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1353\n",
      "Epoch 34/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0366 - mse: 0.0366 - mae: 0.1404 - val_loss: 0.0296 - val_mse: 0.0296 - val_mae: 0.1329\n",
      "Epoch 35/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.0363 - mse: 0.0363 - mae: 0.1390 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1312\n",
      "Epoch 36/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0360 - mse: 0.0360 - mae: 0.1382 - val_loss: 0.0287 - val_mse: 0.0287 - val_mae: 0.1299\n",
      "Epoch 37/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0357 - mse: 0.0357 - mae: 0.1373 - val_loss: 0.0284 - val_mse: 0.0284 - val_mae: 0.1294\n",
      "Epoch 38/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.0354 - mse: 0.0354 - mae: 0.1367 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1288\n",
      "Epoch 39/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0351 - mse: 0.0351 - mae: 0.1359 - val_loss: 0.0278 - val_mse: 0.0278 - val_mae: 0.1277\n",
      "Epoch 40/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0348 - mse: 0.0348 - mae: 0.1351 - val_loss: 0.0275 - val_mse: 0.0275 - val_mae: 0.1271\n",
      "Epoch 41/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0345 - mse: 0.0345 - mae: 0.1344 - val_loss: 0.0272 - val_mse: 0.0272 - val_mae: 0.1263\n",
      "Epoch 42/150\n",
      "236/236 [==============================] - 0s 64us/sample - loss: 0.0342 - mse: 0.0342 - mae: 0.1337 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1257\n",
      "Epoch 43/150\n",
      "236/236 [==============================] - 0s 55us/sample - loss: 0.0339 - mse: 0.0339 - mae: 0.1329 - val_loss: 0.0266 - val_mse: 0.0266 - val_mae: 0.1247\n",
      "Epoch 44/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0336 - mse: 0.0336 - mae: 0.1321 - val_loss: 0.0263 - val_mse: 0.0263 - val_mae: 0.1241\n",
      "Epoch 45/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0333 - mse: 0.0333 - mae: 0.1314 - val_loss: 0.0260 - val_mse: 0.0260 - val_mae: 0.1232\n",
      "Epoch 46/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0329 - mse: 0.0329 - mae: 0.1306 - val_loss: 0.0257 - val_mse: 0.0257 - val_mae: 0.1226\n",
      "Epoch 47/150\n",
      "236/236 [==============================] - 0s 59us/sample - loss: 0.0326 - mse: 0.0326 - mae: 0.1298 - val_loss: 0.0254 - val_mse: 0.0254 - val_mae: 0.1217\n",
      "Epoch 48/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0323 - mse: 0.0323 - mae: 0.1289 - val_loss: 0.0251 - val_mse: 0.0251 - val_mae: 0.1208\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0320 - mse: 0.0320 - mae: 0.1282 - val_loss: 0.0248 - val_mse: 0.0248 - val_mae: 0.1200\n",
      "Epoch 50/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0317 - mse: 0.0317 - mae: 0.1273 - val_loss: 0.0245 - val_mse: 0.0245 - val_mae: 0.1192\n",
      "Epoch 51/150\n",
      "236/236 [==============================] - 0s 51us/sample - loss: 0.0313 - mse: 0.0313 - mae: 0.1265 - val_loss: 0.0242 - val_mse: 0.0242 - val_mae: 0.1184\n",
      "Epoch 52/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0310 - mse: 0.0310 - mae: 0.1257 - val_loss: 0.0239 - val_mse: 0.0239 - val_mae: 0.1177\n",
      "Epoch 53/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0307 - mse: 0.0307 - mae: 0.1248 - val_loss: 0.0235 - val_mse: 0.0235 - val_mae: 0.1167\n",
      "Epoch 54/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0303 - mse: 0.0303 - mae: 0.1239 - val_loss: 0.0232 - val_mse: 0.0232 - val_mae: 0.1159\n",
      "Epoch 55/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0300 - mse: 0.0300 - mae: 0.1231 - val_loss: 0.0229 - val_mse: 0.0229 - val_mae: 0.1152\n",
      "Epoch 56/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0297 - mse: 0.0297 - mae: 0.1223 - val_loss: 0.0226 - val_mse: 0.0226 - val_mae: 0.1144\n",
      "Epoch 57/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0293 - mse: 0.0293 - mae: 0.1212 - val_loss: 0.0222 - val_mse: 0.0222 - val_mae: 0.1128\n",
      "Epoch 58/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0290 - mse: 0.0290 - mae: 0.1202 - val_loss: 0.0218 - val_mse: 0.0218 - val_mae: 0.1117\n",
      "Epoch 59/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0286 - mse: 0.0286 - mae: 0.1191 - val_loss: 0.0214 - val_mse: 0.0214 - val_mae: 0.1104\n",
      "Epoch 60/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0283 - mse: 0.0283 - mae: 0.1180 - val_loss: 0.0210 - val_mse: 0.0210 - val_mae: 0.1089\n",
      "Epoch 61/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0279 - mse: 0.0279 - mae: 0.1168 - val_loss: 0.0206 - val_mse: 0.0206 - val_mae: 0.1075\n",
      "Epoch 62/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0276 - mse: 0.0276 - mae: 0.1158 - val_loss: 0.0202 - val_mse: 0.0202 - val_mae: 0.1063\n",
      "Epoch 63/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0272 - mse: 0.0272 - mae: 0.1148 - val_loss: 0.0199 - val_mse: 0.0199 - val_mae: 0.1053\n",
      "Epoch 64/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0269 - mse: 0.0269 - mae: 0.1139 - val_loss: 0.0195 - val_mse: 0.0195 - val_mae: 0.1044\n",
      "Epoch 65/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0265 - mse: 0.0265 - mae: 0.1129 - val_loss: 0.0191 - val_mse: 0.0191 - val_mae: 0.1030\n",
      "Epoch 66/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0262 - mse: 0.0262 - mae: 0.1120 - val_loss: 0.0189 - val_mse: 0.0189 - val_mae: 0.1026\n",
      "Epoch 67/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0258 - mse: 0.0258 - mae: 0.1111 - val_loss: 0.0186 - val_mse: 0.0186 - val_mae: 0.1015\n",
      "Epoch 68/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0254 - mse: 0.0254 - mae: 0.1101 - val_loss: 0.0182 - val_mse: 0.0182 - val_mae: 0.1003\n",
      "Epoch 69/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0251 - mse: 0.0251 - mae: 0.1091 - val_loss: 0.0180 - val_mse: 0.0180 - val_mae: 0.0995\n",
      "Epoch 70/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0247 - mse: 0.0247 - mae: 0.1080 - val_loss: 0.0176 - val_mse: 0.0176 - val_mae: 0.0980\n",
      "Epoch 71/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0244 - mse: 0.0244 - mae: 0.1070 - val_loss: 0.0172 - val_mse: 0.0172 - val_mae: 0.0970\n",
      "Epoch 72/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0241 - mse: 0.0241 - mae: 0.1060 - val_loss: 0.0169 - val_mse: 0.0169 - val_mae: 0.0958\n",
      "Epoch 73/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0237 - mse: 0.0237 - mae: 0.1049 - val_loss: 0.0166 - val_mse: 0.0166 - val_mae: 0.0946\n",
      "Epoch 74/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0234 - mse: 0.0234 - mae: 0.1039 - val_loss: 0.0163 - val_mse: 0.0163 - val_mae: 0.0936\n",
      "Epoch 75/150\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429 - mae: 0.146 - 0s 42us/sample - loss: 0.0230 - mse: 0.0230 - mae: 0.1029 - val_loss: 0.0159 - val_mse: 0.0159 - val_mae: 0.0921\n",
      "Epoch 76/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0227 - mse: 0.0227 - mae: 0.1017 - val_loss: 0.0156 - val_mse: 0.0156 - val_mae: 0.0907\n",
      "Epoch 77/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0224 - mse: 0.0224 - mae: 0.1005 - val_loss: 0.0152 - val_mse: 0.0152 - val_mae: 0.0889\n",
      "Epoch 78/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0221 - mse: 0.0221 - mae: 0.0995 - val_loss: 0.0149 - val_mse: 0.0149 - val_mae: 0.0880\n",
      "Epoch 79/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0217 - mse: 0.0217 - mae: 0.0986 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0872\n",
      "Epoch 80/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0214 - mse: 0.0214 - mae: 0.0976 - val_loss: 0.0144 - val_mse: 0.0144 - val_mae: 0.0863\n",
      "Epoch 81/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0211 - mse: 0.0211 - mae: 0.0968 - val_loss: 0.0141 - val_mse: 0.0141 - val_mae: 0.0856\n",
      "Epoch 82/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0208 - mse: 0.0208 - mae: 0.0959 - val_loss: 0.0139 - val_mse: 0.0139 - val_mae: 0.0847\n",
      "Epoch 83/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0205 - mse: 0.0205 - mae: 0.0951 - val_loss: 0.0136 - val_mse: 0.0136 - val_mae: 0.0835\n",
      "Epoch 84/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0202 - mse: 0.0202 - mae: 0.0944 - val_loss: 0.0134 - val_mse: 0.0134 - val_mae: 0.0830\n",
      "Epoch 85/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0198 - mse: 0.0198 - mae: 0.0935 - val_loss: 0.0131 - val_mse: 0.0131 - val_mae: 0.0816\n",
      "Epoch 86/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0196 - mse: 0.0196 - mae: 0.0926 - val_loss: 0.0128 - val_mse: 0.0128 - val_mae: 0.0807\n",
      "Epoch 87/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0193 - mse: 0.0193 - mae: 0.0919 - val_loss: 0.0126 - val_mse: 0.0126 - val_mae: 0.0800\n",
      "Epoch 88/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0190 - mse: 0.0190 - mae: 0.0912 - val_loss: 0.0124 - val_mse: 0.0124 - val_mae: 0.0787\n",
      "Epoch 89/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0187 - mse: 0.0187 - mae: 0.0903 - val_loss: 0.0121 - val_mse: 0.0121 - val_mae: 0.0773\n",
      "Epoch 90/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0184 - mse: 0.0184 - mae: 0.0893 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0759\n",
      "Epoch 91/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0182 - mse: 0.0182 - mae: 0.0884 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0742\n",
      "Epoch 92/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0179 - mse: 0.0179 - mae: 0.0875 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0730\n",
      "Epoch 93/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0177 - mse: 0.0177 - mae: 0.0868 - val_loss: 0.0111 - val_mse: 0.0111 - val_mae: 0.0727\n",
      "Epoch 94/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0174 - mse: 0.0174 - mae: 0.0862 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0718\n",
      "Epoch 95/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0172 - mse: 0.0172 - mae: 0.0856 - val_loss: 0.0108 - val_mse: 0.0108 - val_mae: 0.0711\n",
      "Epoch 96/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0170 - mse: 0.0170 - mae: 0.0851 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0709\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0167 - mse: 0.0167 - mae: 0.0846 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0701\n",
      "Epoch 98/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0165 - mse: 0.0165 - mae: 0.0839 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0688\n",
      "Epoch 99/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0163 - mse: 0.0163 - mae: 0.0832 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0681\n",
      "Epoch 100/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0161 - mse: 0.0161 - mae: 0.0827 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0675\n",
      "Epoch 101/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0159 - mse: 0.0159 - mae: 0.0824 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0674\n",
      "Epoch 102/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0157 - mse: 0.0157 - mae: 0.0820 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0670\n",
      "Epoch 103/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0155 - mse: 0.0155 - mae: 0.0816 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0663\n",
      "Epoch 104/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0153 - mse: 0.0153 - mae: 0.0810 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0655\n",
      "Epoch 105/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0151 - mse: 0.0151 - mae: 0.0808 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0655\n",
      "Epoch 106/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0149 - mse: 0.0149 - mae: 0.0804 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0646\n",
      "Epoch 107/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0148 - mse: 0.0148 - mae: 0.0798 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0636\n",
      "Epoch 108/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0146 - mse: 0.0146 - mae: 0.0795 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0634\n",
      "Epoch 109/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0145 - mse: 0.0145 - mae: 0.0790 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0626\n",
      "Epoch 110/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0143 - mse: 0.0143 - mae: 0.0784 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0618\n",
      "Epoch 111/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0142 - mse: 0.0142 - mae: 0.0781 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0617\n",
      "Epoch 112/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0140 - mse: 0.0140 - mae: 0.0779 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0617\n",
      "Epoch 113/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0139 - mse: 0.0139 - mae: 0.0780 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0621\n",
      "Epoch 114/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0138 - mse: 0.0138 - mae: 0.0780 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0620\n",
      "Epoch 115/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0136 - mse: 0.0136 - mae: 0.0776 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0613\n",
      "Epoch 116/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0135 - mse: 0.0135 - mae: 0.0772 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0612\n",
      "Epoch 117/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0134 - mse: 0.0134 - mae: 0.0770 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0609\n",
      "Epoch 118/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0133 - mse: 0.0133 - mae: 0.0768 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0608\n",
      "Epoch 119/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0131 - mse: 0.0131 - mae: 0.0765 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0608\n",
      "Epoch 120/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0130 - mse: 0.0130 - mae: 0.0764 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0608\n",
      "Epoch 121/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0130 - mse: 0.0130 - mae: 0.0764 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0610\n",
      "Epoch 122/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0128 - mse: 0.0128 - mae: 0.0760 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0602\n",
      "Epoch 123/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0127 - mse: 0.0127 - mae: 0.0754 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0602\n",
      "Epoch 124/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0127 - mse: 0.0127 - mae: 0.0754 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0605\n",
      "Epoch 125/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0126 - mse: 0.0126 - mae: 0.0756 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0609\n",
      "Epoch 126/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0125 - mse: 0.0125 - mae: 0.0757 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0610\n",
      "Epoch 127/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0124 - mse: 0.0124 - mae: 0.0753 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0602\n",
      "Epoch 128/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0123 - mse: 0.0123 - mae: 0.0749 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0601\n",
      "Epoch 129/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0123 - mse: 0.0123 - mae: 0.0748 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0605\n",
      "Epoch 130/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0122 - mse: 0.0122 - mae: 0.0751 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0612\n",
      "Epoch 131/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0121 - mse: 0.0121 - mae: 0.0752 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0609\n",
      "Epoch 132/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0121 - mse: 0.0121 - mae: 0.0750 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0609\n",
      "Epoch 133/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0120 - mse: 0.0120 - mae: 0.0748 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0607\n",
      "Epoch 134/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0120 - mse: 0.0120 - mae: 0.0745 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0605\n",
      "Epoch 135/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0119 - mse: 0.0119 - mae: 0.0744 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0608\n",
      "Epoch 136/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0118 - mse: 0.0118 - mae: 0.0745 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0611\n",
      "Epoch 137/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0118 - mse: 0.0118 - mae: 0.0747 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0613\n",
      "Epoch 138/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0118 - mse: 0.0118 - mae: 0.0747 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0611\n",
      "Epoch 139/150\n",
      "236/236 [==============================] - 0s 38us/sample - loss: 0.0117 - mse: 0.0117 - mae: 0.0748 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0618\n",
      "Epoch 140/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0117 - mse: 0.0117 - mae: 0.0749 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0617\n",
      "Epoch 141/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0116 - mse: 0.0116 - mae: 0.0746 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0615\n",
      "Epoch 142/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0115 - mse: 0.0115 - mae: 0.0744 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0615\n",
      "Epoch 143/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0115 - mse: 0.0115 - mae: 0.0744 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0617\n",
      "Epoch 144/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0115 - mse: 0.0115 - mae: 0.0743 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0616\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0114 - mse: 0.0114 - mae: 0.0742 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0616\n",
      "Epoch 146/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0114 - mse: 0.0114 - mae: 0.0740 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0616\n",
      "Epoch 147/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0113 - mse: 0.0113 - mae: 0.0740 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0617\n",
      "Epoch 148/150\n",
      "236/236 [==============================] - 0s 47us/sample - loss: 0.0113 - mse: 0.0113 - mae: 0.0741 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0618\n",
      "Epoch 149/150\n",
      "236/236 [==============================] - 0s 42us/sample - loss: 0.0113 - mse: 0.0113 - mae: 0.0740 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0617\n",
      "Epoch 150/150\n",
      "236/236 [==============================] - 0s 34us/sample - loss: 0.0112 - mse: 0.0112 - mae: 0.0737 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0616\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# plotting loss function with epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
